# HW3

## Parsing and Preprocessing
* https://ria.ru/
* https://lenta.ru/
### Парсинг велся по следующим темам:
- Экономика
- Общество/Россия
- Наука и техника
- Силовые структуры
- Туризм/Путешествия
- Спорт
- Бывший СССР  
Общее кол-во статей около 25к, полсле предобработки около 22к  
### Предобработка включала в себя:  
- лемматизация посредством pymystem3
- удаление текстов с менее чем 74 словами и более чем 696 словами
- удаление дубликатов явных и не близких (jaccard distance)

## Обучение моделей
### LogisticRegression
  
|векторизация | предобработка | подбор параметров | f1_score |
|:----------- |:------------- |:----------------- |:-------- |
|bow | Нет | нет | 0.91068 |
|bow | stop_words + lemma | Нет | 0.91392 |
|bow | stop_words + lemma | Да | 0.91997 |

### CatBoost
|векторизация | предобработка | подбор параметров | f1_score |
|:----------- |:------------- |:----------------- |:-------- |
|чистый текст | Нет | Да | 0.855261 |
|предобработанный текст | stop_words + lemma | Да | 0.867981 |

## summarization
Логистическая регрессия отработала лучше чем boosting. вероятно связанно с тем, что у нас практически отсутствуют табличные данные.  
Для простых текстов логистическая регрессия работает достаточно хорошо, при условии затрат. 

P.S.
Улучшения логистической регресии:
- Добавить предобработку текста, например очистка паттерное
- Сжатие sparse_vector в статичный размер, в этом случае можно догененить различный features, кол-во слов, абзадцев, фото и т.п.
- blending
- обучать solo_model под каждый из классов
- в качестве входных text2vec использовать embedding слой с bert-like_model

Улучшения CatBoost:
- Использовать статичный размер эмбеддинга
- Feature_Eng, перебор всевозможных фич, кол-во пункутации, слов, предложений и т.п.
- При наличии фото, можно взять и его векторизовать.
- подобрать встроенные параметры для токенизации текста.

Также можно попробовать векторизовать word2vec, TF-IDF, Bert_enbedding