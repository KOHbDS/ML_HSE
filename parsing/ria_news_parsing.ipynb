{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re, json, random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(json_list, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding=\"utf-8\") as output_file:\n",
    "        for sample in json_list:\n",
    "            json_line = json.dumps(sample, ensure_ascii=False)\n",
    "            #json.dumps(sample, output_file)\n",
    "            output_file.write(json_line + '\\n')\n",
    "\n",
    "def read_jsonl(read_file_path):\n",
    "    with open(read_file_path, encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(i) for i in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ria_topics = {\n",
    "    \"economy\": 'Экономика', \n",
    "    \"society\" : 'Общество/Россия', \n",
    "    \"science\": 'Наука и техника', \n",
    "    \"defense_safety\": 'Силовые структуры', \n",
    "    \"tourism_news\": 'Туризм/Путешествия'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Article:\n",
    "    id: str = None\n",
    "    url: str = None\n",
    "    title: str = None\n",
    "    subtitle: str = None\n",
    "    topic: str = None\n",
    "    content: str = None\n",
    "    datetime: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "chrome_options.add_argument(\"headless\")\n",
    "chrome_options.add_argument(\"no-sandbox\")\n",
    "chrome_options.add_argument(\"disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://ria.ru\"\n",
    "today = datetime.today()\n",
    "start_date = datetime(2023, 1, 1)\n",
    "work_path = Path('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_html(BASE_URL, topic, step):\n",
    "    try:\n",
    "        news = []\n",
    "        URL = BASE_URL + '/' + topic\n",
    "        driver.get(URL)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # push to list 20 next articles\n",
    "        driver.execute_script(\n",
    "            \"document.getElementsByClassName('list-more')[0].click()\"\n",
    "        )\n",
    "        time.sleep(1)\n",
    "        # scroll page to automatically load more articles\n",
    "        for i in tqdm(range(1500), leave=False):\n",
    "            try:\n",
    "                driver.execute_script(\n",
    "                    f\"window.scrollTo(0, document.body.scrollHeight - 1200)\"\n",
    "                )\n",
    "                time.sleep(0.15)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # find all pages\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        scope = soup.find(\n",
    "            \"div\", {\"class\": \"list\", \"itemtype\": \"http://schema.org/ItemList\"}\n",
    "        )\n",
    "        news += scope.find_all(\"div\", {\"class\": \"list-item\"})\n",
    "    except:\n",
    "        pass\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse_page(page, topic):\n",
    "    \"\"\"Extract from page desired fields\"\"\"\n",
    "\n",
    "    # Create article data class object\n",
    "    article = Article()\n",
    "    article.topic = ria_topics[topic]\n",
    "\n",
    "    # article url\n",
    "    article.url = page.find(\"a\", {\"class\": \"list-item__image\"})[\"href\"]\n",
    "\n",
    "    # article id\n",
    "    s = re.findall(r\"\\d+.html\", article.url)[0]\n",
    "    article.id = s[: s.find(\".\")]\n",
    "\n",
    "    # load page\n",
    "    driver.get(article.url)\n",
    "    time.sleep(1)\n",
    "    html = driver.page_source\n",
    "\n",
    "    # article source\n",
    "    source = article.url[8 : article.url.find(\".\")]\n",
    "\n",
    "    # article object\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    obj = soup.find(\n",
    "        \"div\",\n",
    "        {\n",
    "            \"class\": lambda x: x and (x.find(f\"article m-article m-{source}\") > -1),\n",
    "            \"data-article-id\": article.id,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if not obj:\n",
    "        obj = soup.find(\n",
    "            \"div\",\n",
    "            {\n",
    "                \"class\": lambda x: x and (x.find(f\"article m-video m-{source}\") > -1),\n",
    "                \"data-article-id\": article.id,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # process article title\n",
    "    title = obj.find(\"div\", {\"class\": \"article__title\"})\n",
    "    title_2 = obj.find(\"h1\", {\"class\": \"article__title\"})\n",
    "\n",
    "    if title:\n",
    "        article.title = title.text\n",
    "    else:\n",
    "        article.title = title_2.text if title_2 else \"\"\n",
    "\n",
    "    # article subtitle\n",
    "    subtitle = obj.find(\"h1\", {\"class\": \"article__second-title\"})\n",
    "    article.subtitle = subtitle.text if subtitle else \"\"\n",
    "\n",
    "    # article content\n",
    "    article.content = obj.find(\n",
    "        \"div\", {\"class\": \"article__body js-mediator-article mia-analytics\"}\n",
    "    ).text\n",
    "\n",
    "    # article datetime\n",
    "    article.datetime = obj.find(\"div\", {\"class\": \"article__info-date\"}).find(\"a\").text\n",
    "\n",
    "    # article number of views\n",
    "    # article.views = int(obj.find('span', {'class': 'statistic__item m-views'}).text)\n",
    "\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefa5fd68c73462990d7e7f6976c8e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_save = work_path.joinpath('data/ria_news')\n",
    "for topic, topic_ru in ria_topics.items():\n",
    "    topic_news = get_topic_html(BASE_URL, topic)\n",
    "    random.shuffle(topic_news)\n",
    "    iterator2 = tqdm(topic_news, total=len(topic_news))\n",
    "    parsed_topic_news = [await parse_page(page, topic) for page in iterator2]\n",
    "    \n",
    "    save_jsonl([i.__dict__ for i in parsed_topic_news if i], path_to_save.joinpath('ria_{topic}.jsonl').__str__())\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
